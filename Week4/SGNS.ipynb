
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SGNS.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JsAbi-i8v74c",
        "colab_type": "text"
      },
      "source": [
        "# **Neural Word Embedding**\n",
        "\n",
        "> **Word2Vec, Continuous Bag of Word (CBOW)**\n",
        "\n",
        "> **Word2Vec, Skip-gram with negative sampling (SGNS)**\n",
        "\n",
        "> **Main key point: Distributional Hypothesis**\n",
        "\n",
        "> Goal: Predict the context words from a given word\n",
        "\n",
        "# **How to implement SGNS Algorithm:**\n",
        "\n",
        "\n",
        "1.   Data preprocessing\n",
        "2.   Hyperparameters\n",
        "3.   Training Data\n",
        "4.   Model Fitting\n",
        "5.   Inference/Prediction the testing samples\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "sT6SltefXAJf"
      },
      "source": [
        "### **Main Class**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZdXqCMgWYYtd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from collections import defaultdict\n",
        "import numpy as np\n",
        "class word2vec():\n",
        "\n",
        "  def __init__(self):\n",
        "    self.n = hyperparameters['n']\n",
        "    self.learningrate = hyperparameters['learning_rate']\n",
        "    self.epochs = hyperparameters['epochs']\n",
        "    self.windowsize = hyperparameters['window_size']\n",
        "\n",
        " \n",
        "  def word2onehot(self, word):\n",
        "    word_vector =  np.zeros(self.vocabulary_count)\n",
        "    word_index = self.word_index[word]\n",
        "    word_vector[word_index] = 1\n",
        "    return word_vector\n",
        "\n",
        "  def generate_training_data(self, setting, corpus):\n",
        "    word_counts = defaultdict(int)\n",
        "    # print(word_counts)\n",
        "    for row in corpus:\n",
        "      for token in row:\n",
        "        word_counts[token] +=1 \n",
        "    #print(word_counts)\n",
        "    self.vocabulary_count = len(word_counts.keys())\n",
        "    #print(self.vocabulary_count)\n",
        "    self.words_list = list(word_counts.keys())\n",
        "    #print(self.words_list)\n",
        "    self.word_index = dict((word, i) for i, word in enumerate(self.words_list))\n",
        "    #print(self.word_index)\n",
        "    self.index_word = dict((i, word) for i, word in enumerate(self.words_list))\n",
        "    #print(self.index_word)\n",
        "\n",
        "    training_data = []\n",
        "    for sentence in corpus:\n",
        "      sentence_length = len(sentence)\n",
        "      for i , word in enumerate(sentence):\n",
        "        word_target = self.word2onehot(sentence[i])\n",
        "        #print(word_target)\n",
        "        word_context = []\n",
        "        for j in range(i - self.windowsize, i + self.windowsize + 1):\n",
        "          if j !=i and  j <= sentence_length - 1 and j >= 0:\n",
        "            word_context.append(self.word2onehot(sentence[j]))\n",
        "            # print(word_context)\n",
        "        training_data.append([word_target, word_context])\n",
        "                              \n",
        "      return np.array(training_data)\n",
        "    \n",
        "  def model_training(self, training_data):\n",
        "      self.w1 = np.random.uniform(-1, 1, (self.vocabulary_count, self.n))\n",
        "      self.w2 = np.random.uniform(-1, 1, (self.n, self.vocabulary_count))\n",
        "      for i in range(0, self.epochs):\n",
        "        # self.loss = 0\n",
        "        for word_target, word_context in training_data:\n",
        "          h, u, y_pred= self.forward_pass(word_target)\n",
        "          # print(y_pred)\n",
        "   \n",
        "  def forward_pass(self, x):\n",
        "      h = np.dot(self.w1.T, x)\n",
        "      u = np.dot(self.w2.T, h)\n",
        "      y_pred= self.softmax(u)\n",
        "      return h, u, y_pred\n",
        "    \n",
        "    \n",
        "  def softmax(self, x):\n",
        "      e = np.exp(x - np.max(x))\n",
        "      return e / e.sum(axis=0)\n",
        "\n",
        "  def word_vector(self, word):\n",
        "    word_index = self.word_index[word]\n",
        "    word_vector = self.w1[word_index]\n",
        "    return word_vector\n",
        "\n",
        "  def similar_vectors(self, word, n):\n",
        "    vw1 = self.word_vector(word)\n",
        "    word_similar={}\n",
        "    for i in range(self.vocabulary_count):\n",
        "      vw2 = self.w1[i]\n",
        "      theta_nom= np.dot(vw1, vw2)\n",
        "      theta_denom = np.linalg.norm(vw1) * np.linalg.norm(vw2)\n",
        "      theta = theta_nom / theta_denom\n",
        "      # print(theta)\n",
        "\n",
        "      word = self.index_word[i]\n",
        "      word_similar[word] = theta\n",
        "    # {k: v for k, v in sorted(x.items(), key=lambda item: item[1])}\n",
        "    words_sorted = sorted(word_similar.items(), key=lambda ss: ss[1], reverse=True)\n",
        "    for word, similar in words_sorted[:n]:\n",
        "      print(word, similar)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OQDL8ZdH4Q6k",
        "colab_type": "text"
      },
      "source": [
        "### **1.Data PreProcessing**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iJayMXuIvN9t",
        "colab_type": "code",
        "outputId": "7b69dd10-0ecf-4775-b829-395c4470becf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Define the mini corpus\n",
        "document = \"A combination of Machine Learning and Natural Language Processing works well\"\n",
        "\n",
        "# Tokenizing and build a vocabulary\n",
        "corpus = [[]]\n",
        "for token in document.split():\n",
        "  corpus[0].append(token.lower())\n",
        "\n",
        "print(corpus)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['a', 'combination', 'of', 'machine', 'learning', 'and', 'natural', 'language', 'processing', 'works', 'well']]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-vNBazTV56jb",
        "colab_type": "text"
      },
      "source": [
        "### **2. Hyperparameters**"
      ]
    },
    {