
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SGNS.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JsAbi-i8v74c",
        "colab_type": "text"
      },
      "source": [
        "# **Neural Word Embedding**\n",
        "\n",
        "> **Word2Vec, Continuous Bag of Word (CBOW)**\n",
        "\n",
        "> **Word2Vec, Skip-gram with negative sampling (SGNS)**\n",
        "\n",
        "> **Main key point: Distributional Hypothesis**\n",
        "\n",
        "> Goal: Predict the context words from a given word\n",
        "\n",
        "# **How to implement SGNS Algorithm:**\n",
        "\n",
        "\n",
        "1.   Data preprocessing\n",
        "2.   Hyperparameters\n",
        "3.   Training Data\n",
        "4.   Model Fitting\n",
        "5.   Inference/Prediction the testing samples\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "sT6SltefXAJf"
      },
      "source": [
        "### **Main Class**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZdXqCMgWYYtd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from collections import defaultdict\n",
        "import numpy as np\n",
        "class word2vec():\n",
        "\n",
        "  def __init__(self):\n",
        "    self.n = hyperparameters['n']\n",
        "    self.learningrate = hyperparameters['learning_rate']\n",
        "    self.epochs = hyperparameters['epochs']\n",
        "    self.windowsize = hyperparameters['window_size']\n",
        "\n",
        " \n",
        "  def word2onehot(self, word):\n",
        "    word_vector =  np.zeros(self.vocabulary_count)\n",
        "    word_index = self.word_index[word]\n",
        "    word_vector[word_index] = 1\n",
        "    return word_vector\n",
        "\n",
        "  def generate_training_data(self, setting, corpus):\n",
        "    word_counts = defaultdict(int)\n",
        "    # print(word_counts)\n",
        "    for row in corpus:\n",
        "      for token in row:\n",
        "        word_counts[token] +=1 \n",
        "    #print(word_counts)\n",
        "    self.vocabulary_count = len(word_counts.keys())\n",
        "    #print(self.vocabulary_count)\n",
        "    self.words_list = list(word_counts.keys())\n",
        "    #print(self.words_list)\n",
        "    self.word_index = dict((word, i) for i, word in enumerate(self.words_list))\n",
        "    #print(self.word_index)\n",
        "    self.index_word = dict((i, word) for i, word in enumerate(self.words_list))\n",
        "    #print(self.index_word)\n",
        "\n",
        "    training_data = []\n",
        "    for sentence in corpus:\n",
        "      sentence_length = len(sentence)\n",
        "      for i , word in enumerate(sentence):\n",
        "        word_target = self.word2onehot(sentence[i])\n",